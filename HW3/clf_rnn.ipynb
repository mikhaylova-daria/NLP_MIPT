{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204949, 13) 0.228222107326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000299</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Костюм Didriksons Boardman, размер 100, краги,...</td>\n",
       "      <td>Костюм Didriksons Boardman, в отличном состоян...</td>\n",
       "      <td>{\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000309</td>\n",
       "      <td>Недвижимость</td>\n",
       "      <td>Квартиры</td>\n",
       "      <td>1-к квартира, 44 м², 9/20 эт.</td>\n",
       "      <td>В кирпичном пан.-м доме, продается одноком.-ая...</td>\n",
       "      <td>{\"Тип объявления\":\"Продам\", \"Количество комнат...</td>\n",
       "      <td>2642020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000317</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Поездки на таможню, печать в паспорте</td>\n",
       "      <td>Поездки на таможню гражданам СНГ для пересечен...</td>\n",
       "      <td>{\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid      category                subcategory  \\\n",
       "0  10000010     Транспорт      Автомобили с пробегом   \n",
       "1  10000094   Личные вещи  Одежда, обувь, аксессуары   \n",
       "2  10000299   Личные вещи     Детская одежда и обувь   \n",
       "3  10000309  Недвижимость                   Квартиры   \n",
       "4  10000317        Услуги          Предложения услуг   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Toyota Sera, 1991   \n",
       "1                                   Костюм Steilmann   \n",
       "2  Костюм Didriksons Boardman, размер 100, краги,...   \n",
       "3                      1-к квартира, 44 м², 9/20 эт.   \n",
       "4              Поездки на таможню, печать в паспорте   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "2  Костюм Didriksons Boardman, в отличном состоян...   \n",
       "3  В кирпичном пан.-м доме, продается одноком.-ая...   \n",
       "4  Поездки на таможню гражданам СНГ для пересечен...   \n",
       "\n",
       "                                               attrs    price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...   150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...     1500        NaN   \n",
       "2  {\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...     3000        NaN   \n",
       "3  {\"Тип объявления\":\"Продам\", \"Количество комнат...  2642020        NaN   \n",
       "4  {\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...     1500        0.0   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  \n",
       "2           0           0           0         0         5.49  \n",
       "3           0           1           0         0        22.47  \n",
       "4           1           0           0         0         1.43  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.shape, df.is_blocked.mean()\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Blocked ratio\",df.is_blocked.mean()\n",
    "print \"Count:\",len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#downsample\n",
    "\n",
    "df_0 = df.query('is_blocked==0')\n",
    "df_1 = df.query('is_blocked==1')\n",
    "\n",
    "length = min(len(df_0), len(df_1))\n",
    "frames = [df_0[:length], df_1[:length]]\n",
    "\n",
    "df = pd.concat(frames)\n",
    "\n",
    "print \"Blocked ratio:\",df.is_blocked.mean()\n",
    "print \"Count:\",len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print \"All tests passed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'very_low_ram' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-7cd356d2aa3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#In case your RAM-o-meter is in the red\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mvery_low_ram\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'very_low_ram' is not defined"
     ]
    }
   ],
   "source": [
    "#In case your RAM-o-meter is in the red\n",
    "if very_low_ram:\n",
    "    data = data[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.decode('utf8').lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAFkCAYAAAAKf8APAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12ebb8c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "_=plt.hist(token_counts.values(),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "\n",
    "min_count = 5\n",
    "tokens = [token for token in token_counts.keys() if token_counts[token] > min_count]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119993 521988\n"
     ]
    }
   ],
   "source": [
    "print len(tokens), len(token_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 119994\n"
     ]
    }
   ],
   "source": [
    "print \"# Tokens:\",len(token_to_id)\n",
    "if len(token_to_id) < 30000:\n",
    "    print \"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\"\n",
    "if len(token_to_id) > 1000000:\n",
    "    print \"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.decode('utf8').lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = map(lambda token: token_to_id.get(token,0), tokens)[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (549992, 15)\n",
      "Toyota Sera, 1991 -> [88299 86079 60751     0     0     0     0     0     0     0] ...\n",
      "Костюм Steilmann -> [33490     0     0     0     0     0     0     0     0     0] ...\n",
      "Костюм Didriksons Boardman, размер 100, краги, шап -> [33490 92066     0 49154 99859 33805     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print \"Размер матрицы:\",title_tokens.shape\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print title,'->', tokens[:10],'...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = df[[\"category\",\"subcategory\"]].to_dict('records')\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split into training and test set.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = train_test_split(title_tokens, desc_tokens, df_non_text, target, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data [optional]\n",
    "\n",
    "* The next tab can be used to stash all the essential data matrices and get rid of the rest of the data.\n",
    " * Highly recommended if you have less than 1.5GB RAM left\n",
    "* To do that, you need to first run it with save_prepared_data=True, then restart the notebook and only run this tab with read_prepared_data=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading saved data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "save_prepared_data = False #save\n",
    "read_prepared_data = True #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "if save_prepared_data:\n",
    "    print \"Saving preprocessed data (may take up to 3 minutes)\"\n",
    "\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "        pickle.dump((title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts), fout)\n",
    "    with open(\"token_to_id.pcl\",'w') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print \"готово\"\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print \"Reading saved data...\"\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "        \n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    print \"done\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/physteshka/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subtensor{:int64:}.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor\n",
    "\n",
    "def align_targets(predictions, targets):\n",
    "    \"\"\"Helper function turning a target 1D vector into a column if needed.\n",
    "    This way, combining a network of a single output unit with a target vector\n",
    "    works as expected by most users, not broadcasting outputs against targets.\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : Theano tensor\n",
    "        Expression for the predictions of a neural network.\n",
    "    targets : Theano tensor\n",
    "        Expression or variable for corresponding targets.\n",
    "    Returns\n",
    "    -------\n",
    "    predictions : Theano tensor\n",
    "        The predictions unchanged.\n",
    "    targets : Theano tensor\n",
    "        If `predictions` is a column vector and `targets` is a 1D vector,\n",
    "        returns `targets` turned into a column vector. Otherwise, returns\n",
    "        `targets` unchanged.\n",
    "    \"\"\"\n",
    "    if (getattr(predictions, 'broadcastable', None) == (False, True) and\n",
    "            getattr(targets, 'ndim', None) == 1):\n",
    "        targets = as_theano_expression(targets).dimshuffle(0, 'x')\n",
    "    return predictions, targets\n",
    "\n",
    "def binary_hinge_loss(predictions, targets, delta=1, log_odds=None,\n",
    "                      binary=True):\n",
    "    \"\"\"Computes the binary hinge loss between predictions and targets.\n",
    "    .. math:: L_i = \\\\max(0, \\\\delta - t_i p_i)\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : Theano tensor\n",
    "        Predictions in (0, 1), such as sigmoidal output of a neural network\n",
    "        (or log-odds of predictions depending on `log_odds`).\n",
    "    targets : Theano tensor\n",
    "        Targets in {0, 1} (or in {-1, 1} depending on `binary`), such as\n",
    "        ground truth labels.\n",
    "    delta : scalar, default 1\n",
    "        The hinge loss margin\n",
    "    log_odds : bool, default None\n",
    "        ``False`` if predictions are sigmoid outputs in (0, 1), ``True`` if\n",
    "        predictions are sigmoid inputs, or log-odds. If ``None``, will assume\n",
    "        ``True``, but warn that the default will change to ``False``.\n",
    "    binary : bool, default True\n",
    "        ``True`` if targets are in {0, 1}, ``False`` if they are in {-1, 1}\n",
    "    Returns\n",
    "    -------\n",
    "    Theano tensor\n",
    "        An expression for the element-wise binary hinge loss\n",
    "    Notes\n",
    "    -----\n",
    "    This is an alternative to the binary cross-entropy loss for binary\n",
    "    classification problems.\n",
    "    Note that it is a drop-in replacement only when giving ``log_odds=False``.\n",
    "    Otherwise, it requires log-odds rather than sigmoid outputs. Be aware that\n",
    "    depending on the Theano version, ``log_odds=False`` with a sigmoid\n",
    "    output layer may be less stable than ``log_odds=True`` with a linear layer.\n",
    "    \"\"\"\n",
    "    if log_odds is None:  # pragma: no cover\n",
    "        raise FutureWarning(\n",
    "                \"The `log_odds` argument to `binary_hinge_loss` will change \"\n",
    "                \"its default to `False` in a future version. Explicitly give \"\n",
    "                \"`log_odds=True` to retain current behavior in your code, \"\n",
    "                \"but also check the documentation if this is what you want.\")\n",
    "        log_odds = True\n",
    "    if not log_odds:\n",
    "        predictions = theano.tensor.log(predictions / (1 - predictions))\n",
    "    if binary:\n",
    "        targets = 2 * targets - 1\n",
    "    predictions, targets = align_targets(predictions, targets)\n",
    "    return theano.tensor.nnet.relu(delta - predictions * targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None, title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None, desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None, nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.nonlinearities import *\n",
    "# Descriptions\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp,\n",
    "                                         input_size=len(token_to_id),\n",
    "                                         output_size=128)\n",
    "\n",
    "\n",
    "#reshape from [batch, time, unit] to [batch,unit,time] to allow 1d convolution over time\n",
    "descr_nn = lasagne.layers.DimshuffleLayer(descr_nn, [0,2,1])\n",
    "\n",
    "descr_nn = lasagne.layers.Conv1DLayer(descr_nn, 256, 5)\n",
    "\n",
    "descr_nn = lasagne.layers.MaxPool1DLayer(descr_nn, 2)\n",
    "\n",
    "descr_nn = lasagne.layers.Conv1DLayer(descr_nn, 256, 5)\n",
    "\n",
    "descr_nn = lasagne.layers.GlobalPoolLayer(descr_nn,T.max)\n",
    "\n",
    "\n",
    "# Titles\n",
    "\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp,\n",
    "                                         input_size=len(token_to_id),\n",
    "                                         output_size=128)\n",
    "\n",
    "\n",
    "#reshape from [batch, time, unit] to [batch,unit,time] to allow 1d convolution over time\n",
    "title_nn = lasagne.layers.DimshuffleLayer(title_nn, [0,2,1])\n",
    "\n",
    "title_nn = lasagne.layers.Conv1DLayer(title_nn, 128, 3)\n",
    "\n",
    "title_nn = lasagne.layers.MaxPool1DLayer(title_nn, 2)\n",
    "\n",
    "title_nn = lasagne.layers.Conv1DLayer(title_nn, 128, 3)\n",
    "\n",
    "#pool over time\n",
    "title_nn = lasagne.layers.GlobalPoolLayer(title_nn,T.max)\n",
    "\n",
    "# Non-sequences\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, 256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.ConcatLayer([descr_nn, title_nn, cat_nn])                              \n",
    "\n",
    "nn = lasagne.layers.DenseLayer(nn, num_units=1048)\n",
    "nn = lasagne.layers.DropoutLayer(nn,p=0.3)\n",
    "nn = lasagne.layers.DenseLayer(nn,1,nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lasagne.layers.dense.DenseLayer at 0x112835050>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, W, b, W, b, W, W, b, W, b, W, b, W, b, W, b]\n"
     ]
    }
   ],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn,trainable=True)\n",
    "print weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/physteshka/anaconda2/lib/python2.7/site-packages/lasagne/theano_extensions/conv.py:66: UserWarning: The `image_shape` keyword argument to `tensor.nnet.conv2d` is deprecated, it has been renamed to `input_shape`.\n",
      "  border_mode=border_mode)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = binary_hinge_loss(prediction,target_y, delta = 1, log_odds=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "#updates = lasagne.updates.nesterov_momentum(loss, weights, learning_rate=0.01, momentum=0.9)\n",
    "updates = lasagne.updates.adam(loss, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn,deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = binary_hinge_loss(prediction,target_y,delta = 1, log_odds=True).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr.iloc[excerpt, :] if type(arr) == pd.core.frame.DataFrame else arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch №  0\n",
      "Train:\n",
      "\tloss: 13957.9532824\n",
      "\tacc: 0.577326732673\n",
      "\tauc: 0.639625239873\n",
      "\tap@k: 0.120558553346\n",
      "Val:\n",
      "\tloss: 21190.7984809\n",
      "\tacc: 0.500693069307\n",
      "\tauc: 0.51290291349\n",
      "\tap@k: 0.625509041998\n",
      "Epoch №  1\n",
      "Train:\n",
      "\tloss: 22628.2275853\n",
      "\tacc: 0.585643564356\n",
      "\tauc: 0.613289144099\n",
      "\tap@k: 0.0550574006637\n",
      "Val:\n",
      "\tloss: 67854.5468107\n",
      "\tacc: 0.586633663366\n",
      "\tauc: 0.605280780967\n",
      "\tap@k: 0.86509305513\n",
      "Epoch №  2\n",
      "Train:\n",
      "\tloss: 38082.2905875\n",
      "\tacc: 0.677623762376\n",
      "\tauc: 0.714629119155\n",
      "\tap@k: 0.0640511850745\n",
      "Val:\n",
      "\tloss: 73355.0256193\n",
      "\tacc: 0.627920792079\n",
      "\tauc: 0.669393714388\n",
      "\tap@k: 0.982906681359\n",
      "Epoch №  3\n",
      "Train:\n",
      "\tloss: 11054.0656615\n",
      "\tacc: 0.784257425743\n",
      "\tauc: 0.839039520991\n",
      "\tap@k: 0.295153603199\n",
      "Val:\n",
      "\tloss: 32361.0016196\n",
      "\tacc: 0.729603960396\n",
      "\tauc: 0.755297089363\n",
      "\tap@k: 0.987602971914\n",
      "Epoch №  4\n",
      "Train:\n",
      "\tloss: 9999.85803787\n",
      "\tacc: 0.778316831683\n",
      "\tauc: 0.846044443399\n",
      "\tap@k: 0.325017572049\n",
      "Val:\n",
      "\tloss: 19207.8951786\n",
      "\tacc: 0.737722772277\n",
      "\tauc: 0.782915197404\n",
      "\tap@k: 0.950273875517\n",
      "Epoch №  5\n",
      "Train:\n",
      "\tloss: 57237.8066752\n",
      "\tacc: 0.771089108911\n",
      "\tauc: 0.817230515087\n",
      "\tap@k: 0.139574049132\n",
      "Val:\n",
      "\tloss: 8749.33998479\n",
      "\tacc: 0.743465346535\n",
      "\tauc: 0.868394904474\n",
      "\tap@k: 0.738520017816\n",
      "Epoch №  6\n",
      "Train:\n",
      "\tloss: 8202.46488554\n",
      "\tacc: 0.804158415842\n",
      "\tauc: 0.865766642585\n",
      "\tap@k: 0.509120632835\n",
      "Val:\n",
      "\tloss: 19395.1709995\n",
      "\tacc: 0.809603960396\n",
      "\tauc: 0.85095755932\n",
      "\tap@k: 0.50764726701\n",
      "Epoch №  7\n",
      "Train:\n",
      "\tloss: 2336.5566778\n",
      "\tacc: 0.792772277228\n",
      "\tauc: 0.869541762974\n",
      "\tap@k: 0.681444441298\n",
      "Val:\n",
      "\tloss: 5675.49434209\n",
      "\tacc: 0.799405940594\n",
      "\tauc: 0.858405825385\n",
      "\tap@k: 0.645239812172\n",
      "Epoch №  8\n",
      "Train:\n",
      "\tloss: 2160.19082554\n",
      "\tacc: 0.787722772277\n",
      "\tauc: 0.878670823529\n",
      "\tap@k: 0.743027699715\n",
      "Val:\n",
      "\tloss: 6598.07793359\n",
      "\tacc: 0.784257425743\n",
      "\tauc: 0.850820492623\n",
      "\tap@k: 0.964818935889\n",
      "Epoch №  9\n",
      "Train:\n",
      "\tloss: 7815.25433387\n",
      "\tacc: 0.799900990099\n",
      "\tauc: 0.875671424475\n",
      "\tap@k: 0.511083966602\n",
      "Val:\n",
      "\tloss: 14016.2135589\n",
      "\tacc: 0.822376237624\n",
      "\tauc: 0.855152833952\n",
      "\tap@k: 0.630255111554\n",
      "Epoch №  10\n",
      "Train:\n",
      "\tloss: 5540.10437473\n",
      "\tacc: 0.802079207921\n",
      "\tauc: 0.879272452073\n",
      "\tap@k: 0.528151809837\n",
      "Val:\n",
      "\tloss: 7732.86010439\n",
      "\tacc: 0.766336633663\n",
      "\tauc: 0.851279341297\n",
      "\tap@k: 0.954754005661\n",
      "Epoch №  11\n",
      "Train:\n",
      "\tloss: 10810.651016\n",
      "\tacc: 0.792673267327\n",
      "\tauc: 0.881611530126\n",
      "\tap@k: 0.787156891606\n",
      "Val:\n",
      "\tloss: 33192.801808\n",
      "\tacc: 0.744059405941\n",
      "\tauc: 0.824107948936\n",
      "\tap@k: 0.998477581899\n",
      "Epoch №  12\n",
      "Train:\n",
      "\tloss: 1731.12205903\n",
      "\tacc: 0.807227722772\n",
      "\tauc: 0.897173369827\n",
      "\tap@k: 0.839368549296\n",
      "Val:\n",
      "\tloss: 4899.02587132\n",
      "\tacc: 0.783762376238\n",
      "\tauc: 0.864185312142\n",
      "\tap@k: 0.978146228441\n",
      "Epoch №  13\n",
      "Train:\n",
      "\tloss: 9278.61519178\n",
      "\tacc: 0.768118811881\n",
      "\tauc: 0.846634787314\n",
      "\tap@k: 0.278712749152\n",
      "Val:\n",
      "\tloss: 21276.3902914\n",
      "\tacc: 0.761089108911\n",
      "\tauc: 0.843944154309\n",
      "\tap@k: 0.617782469326\n",
      "Epoch №  14\n",
      "Train:\n",
      "\tloss: 8292.54241161\n",
      "\tacc: 0.81\n",
      "\tauc: 0.900134519521\n",
      "\tap@k: 0.685021595798\n",
      "Val:\n",
      "\tloss: 4262.59737736\n",
      "\tacc: 0.769405940594\n",
      "\tauc: 0.87992279308\n",
      "\tap@k: 0.967640222539\n",
      "Epoch №  15\n",
      "Train:\n",
      "\tloss: 2782.62417514\n",
      "\tacc: 0.779702970297\n",
      "\tauc: 0.883041060198\n",
      "\tap@k: 0.890122478632\n",
      "Val:\n",
      "\tloss: 6330.83544295\n",
      "\tacc: 0.74900990099\n",
      "\tauc: 0.848783011525\n",
      "\tap@k: 0.992289724825\n",
      "Epoch №  16\n",
      "Train:\n",
      "\tloss: 1756.17765322\n",
      "\tacc: 0.785148514851\n",
      "\tauc: 0.889979399201\n",
      "\tap@k: 0.888893592231\n",
      "Val:\n",
      "\tloss: 12672.9330661\n",
      "\tacc: 0.726831683168\n",
      "\tauc: 0.831795418252\n",
      "\tap@k: 0.992730429707\n",
      "Epoch №  17\n",
      "Train:\n",
      "\tloss: 1340.74585843\n",
      "\tacc: 0.794455445545\n",
      "\tauc: 0.894343935521\n",
      "\tap@k: 0.922421465373\n",
      "Val:\n",
      "\tloss: 14192.6099526\n",
      "\tacc: 0.728613861386\n",
      "\tauc: 0.829167407812\n",
      "\tap@k: 0.977805148037\n",
      "Epoch №  18\n",
      "Train:\n",
      "\tloss: 2001.73466837\n",
      "\tacc: 0.798415841584\n",
      "\tauc: 0.896262665872\n",
      "\tap@k: 0.83630433279\n",
      "Val:\n",
      "\tloss: 16425.4612529\n",
      "\tacc: 0.773564356436\n",
      "\tauc: 0.856485496635\n",
      "\tap@k: 0.992712361779\n",
      "Epoch №  19\n",
      "Train:\n",
      "\tloss: 831.830584814\n",
      "\tacc: 0.807920792079\n",
      "\tauc: 0.90470274198\n",
      "\tap@k: 0.939084230024\n",
      "Val:\n",
      "\tloss: 2874.57607702\n",
      "\tacc: 0.782871287129\n",
      "\tauc: 0.872319529771\n",
      "\tap@k: 0.976759360014\n",
      "Epoch №  20\n",
      "Train:\n",
      "\tloss: 23624.3087275\n",
      "\tacc: 0.794851485149\n",
      "\tauc: 0.876815365236\n",
      "\tap@k: 0.522354786801\n",
      "Val:\n",
      "\tloss: 48705.0446128\n",
      "\tacc: 0.763069306931\n",
      "\tauc: 0.83159765937\n",
      "\tap@k: 0.99106519927\n",
      "Epoch №  21\n",
      "Train:\n",
      "\tloss: 11686.6368717\n",
      "\tacc: 0.797524752475\n",
      "\tauc: 0.892528576914\n",
      "\tap@k: 0.730778958967\n",
      "Val:\n",
      "\tloss: 6336.73914127\n",
      "\tacc: 0.761386138614\n",
      "\tauc: 0.855027942994\n",
      "\tap@k: 0.990014514102\n",
      "Epoch №  22\n",
      "Train:\n",
      "\tloss: 999.049432191\n",
      "\tacc: 0.798910891089\n",
      "\tauc: 0.899065036431\n",
      "\tap@k: 0.919110806269\n",
      "Val:\n",
      "\tloss: 4408.12829655\n",
      "\tacc: 0.795544554455\n",
      "\tauc: 0.886152333333\n",
      "\tap@k: 0.982653124865\n",
      "Epoch №  23\n",
      "Train:\n",
      "\tloss: 8021.39504473\n",
      "\tacc: 0.808514851485\n",
      "\tauc: 0.900789292805\n",
      "\tap@k: 0.824369386055\n",
      "Val:\n",
      "\tloss: 5091.6692188\n",
      "\tacc: 0.795346534653\n",
      "\tauc: 0.89654731508\n",
      "\tap@k: 0.973661275276\n",
      "Epoch №  24\n",
      "Train:\n",
      "\tloss: 3427.92693557\n",
      "\tacc: 0.815940594059\n",
      "\tauc: 0.902393172254\n",
      "\tap@k: 0.779343611235\n",
      "Val:\n",
      "\tloss: 8278.26336259\n",
      "\tacc: 0.776930693069\n",
      "\tauc: 0.854424948662\n",
      "\tap@k: 0.992917030754\n",
      "Epoch №  25\n",
      "Train:\n",
      "\tloss: 1527.03479219\n",
      "\tacc: 0.812277227723\n",
      "\tauc: 0.906833029909\n",
      "\tap@k: 0.919411292337\n",
      "Val:\n",
      "\tloss: 3690.4576752\n",
      "\tacc: 0.791782178218\n",
      "\tauc: 0.875552548248\n",
      "\tap@k: 0.983085931226\n",
      "Epoch №  26\n",
      "Train:\n",
      "\tloss: 10612.3897782\n",
      "\tacc: 0.803366336634\n",
      "\tauc: 0.891192916361\n",
      "\tap@k: 0.629379596663\n",
      "Val:\n",
      "\tloss: 8929.60629709\n",
      "\tacc: 0.786831683168\n",
      "\tauc: 0.869385786098\n",
      "\tap@k: 0.59228926029\n",
      "Epoch №  27\n",
      "Train:\n",
      "\tloss: 627.446861518\n",
      "\tacc: 0.813663366337\n",
      "\tauc: 0.90837891996\n",
      "\tap@k: 0.854274687152\n",
      "Val:\n",
      "\tloss: 4403.066526\n",
      "\tacc: 0.80495049505\n",
      "\tauc: 0.896943862315\n",
      "\tap@k: 0.98597167471\n",
      "Epoch №  28\n",
      "Train:\n",
      "\tloss: 918.267101789\n",
      "\tacc: 0.823267326733\n",
      "\tauc: 0.912952531503\n",
      "\tap@k: 0.906237667169\n",
      "Val:\n",
      "\tloss: 5353.45563355\n",
      "\tacc: 0.771584158416\n",
      "\tauc: 0.860189058594\n",
      "\tap@k: 0.98315439207\n",
      "Epoch №  29\n",
      "Train:\n",
      "\tloss: 1825.2396221\n",
      "\tacc: 0.812277227723\n",
      "\tauc: 0.906693017438\n",
      "\tap@k: 0.920411852098\n",
      "Val:\n",
      "\tloss: 4657.61921353\n",
      "\tacc: 0.795940594059\n",
      "\tauc: 0.877507104554\n",
      "\tap@k: 0.978086839831\n",
      "Epoch №  30\n",
      "Train:\n",
      "\tloss: 786.627097152\n",
      "\tacc: 0.811881188119\n",
      "\tauc: 0.908329806694\n",
      "\tap@k: 0.941731420051\n",
      "Val:\n",
      "\tloss: 2157.11769211\n",
      "\tacc: 0.810891089109\n",
      "\tauc: 0.887335344825\n",
      "\tap@k: 0.792215335032\n",
      "Epoch №  31\n",
      "Train:\n",
      "\tloss: 1535.46384241\n",
      "\tacc: 0.816831683168\n",
      "\tauc: 0.909166156911\n",
      "\tap@k: 0.866499167737\n",
      "Val:\n",
      "\tloss: 2986.79957389\n",
      "\tacc: 0.808415841584\n",
      "\tauc: 0.900980101739\n",
      "\tap@k: 0.899016514468\n",
      "Epoch №  32\n",
      "Train:\n",
      "\tloss: 7827.74053908\n",
      "\tacc: 0.806831683168\n",
      "\tauc: 0.89596315793\n",
      "\tap@k: 0.786915555277\n",
      "Val:\n",
      "\tloss: 10877.0871881\n",
      "\tacc: 0.780198019802\n",
      "\tauc: 0.862351914078\n",
      "\tap@k: 0.981383615441\n",
      "Epoch №  33\n",
      "Train:\n",
      "\tloss: 851.681804707\n",
      "\tacc: 0.806435643564\n",
      "\tauc: 0.905355134517\n",
      "\tap@k: 0.970970627354\n",
      "Val:\n",
      "\tloss: 2799.92181258\n",
      "\tacc: 0.79603960396\n",
      "\tauc: 0.892479495494\n",
      "\tap@k: 0.980398190748\n",
      "Epoch №  34\n",
      "Train:\n",
      "\tloss: 495.725837013\n",
      "\tacc: 0.814653465347\n",
      "\tauc: 0.90525585954\n",
      "\tap@k: 0.931238109687\n",
      "Val:\n",
      "\tloss: 3957.87266685\n",
      "\tacc: 0.789702970297\n",
      "\tauc: 0.865785615438\n",
      "\tap@k: 0.980166203551\n",
      "Epoch №  35\n",
      "Train:\n",
      "\tloss: 3858.31436033\n",
      "\tacc: 0.797821782178\n",
      "\tauc: 0.89303139526\n",
      "\tap@k: 0.837288608384\n",
      "Val:\n",
      "\tloss: 4670.11204488\n",
      "\tacc: 0.799603960396\n",
      "\tauc: 0.886480865055\n",
      "\tap@k: 0.996436740807\n",
      "Epoch №  36\n",
      "Train:\n",
      "\tloss: 1007.17081093\n",
      "\tacc: 0.808118811881\n",
      "\tauc: 0.906481764353\n",
      "\tap@k: 0.907374125407\n",
      "Val:\n",
      "\tloss: 3184.16683121\n",
      "\tacc: 0.810891089109\n",
      "\tauc: 0.902492961226\n",
      "\tap@k: 0.99001756654\n",
      "Epoch №  37\n",
      "Train:\n",
      "\tloss: 1211.79566289\n",
      "\tacc: 0.80198019802\n",
      "\tauc: 0.900386488145\n",
      "\tap@k: 0.965168490618\n",
      "Val:\n",
      "\tloss: 2645.99099273\n",
      "\tacc: 0.769504950495\n",
      "\tauc: 0.862047442682\n",
      "\tap@k: 0.746090173796\n",
      "Epoch №  38\n",
      "Train:\n",
      "\tloss: 869.285549921\n",
      "\tacc: 0.811188118812\n",
      "\tauc: 0.905731863645\n",
      "\tap@k: 0.897175821602\n",
      "Val:\n",
      "\tloss: 1964.84702759\n",
      "\tacc: 0.800594059406\n",
      "\tauc: 0.891670759271\n",
      "\tap@k: 0.949461157081\n",
      "Epoch №  39\n",
      "Train:\n",
      "\tloss: 232.45721028\n",
      "\tacc: 0.802376237624\n",
      "\tauc: 0.901078955341\n",
      "\tap@k: 0.935419736867\n",
      "Val:\n",
      "\tloss: 2176.1506324\n",
      "\tacc: 0.767722772277\n",
      "\tauc: 0.874158285004\n",
      "\tap@k: 0.995909640637\n",
      "Epoch №  40\n",
      "Train:\n",
      "\tloss: 2796.03695257\n",
      "\tacc: 0.798514851485\n",
      "\tauc: 0.897069440739\n",
      "\tap@k: 0.880502902013\n",
      "Val:\n",
      "\tloss: 3683.09009836\n",
      "\tacc: 0.783861386139\n",
      "\tauc: 0.873473147093\n",
      "\tap@k: 0.657742575944\n",
      "Epoch №  41\n",
      "Train:\n",
      "\tloss: 624.991962044\n",
      "\tacc: 0.802871287129\n",
      "\tauc: 0.900549469747\n",
      "\tap@k: 0.85171620405\n",
      "Val:\n",
      "\tloss: 1669.185426\n",
      "\tacc: 0.764455445545\n",
      "\tauc: 0.870631844431\n",
      "\tap@k: 0.804029890441\n",
      "Epoch №  42\n",
      "Train:\n",
      "\tloss: 557.005749433\n",
      "\tacc: 0.78396039604\n",
      "\tauc: 0.890112867231\n",
      "\tap@k: 0.930084964762\n",
      "Val:\n",
      "\tloss: 1365.68189299\n",
      "\tacc: 0.791782178218\n",
      "\tauc: 0.874129982344\n",
      "\tap@k: 0.878837273113\n",
      "Epoch №  43\n",
      "Train:\n",
      "\tloss: 746.861862129\n",
      "\tacc: 0.782178217822\n",
      "\tauc: 0.883987189485\n",
      "\tap@k: 0.908371929623\n",
      "Val:\n",
      "\tloss: 2370.7885846\n",
      "\tacc: 0.772772277228\n",
      "\tauc: 0.860510909169\n",
      "\tap@k: 0.785816751036\n",
      "Epoch №  44\n",
      "Train:\n",
      "\tloss: 248.796619084\n",
      "\tacc: 0.781386138614\n",
      "\tauc: 0.86834299619\n",
      "\tap@k: 0.936523207105\n",
      "Val:\n",
      "\tloss: 860.168319119\n",
      "\tacc: 0.767227722772\n",
      "\tauc: 0.847861677343\n",
      "\tap@k: 0.922158356628\n",
      "Epoch №  45\n",
      "Train:\n",
      "\tloss: 77.1400080301\n",
      "\tacc: 0.770495049505\n",
      "\tauc: 0.863449964662\n",
      "\tap@k: 0.983007266794\n",
      "Val:\n",
      "\tloss: 1921.3680465\n",
      "\tacc: 0.762871287129\n",
      "\tauc: 0.844135417974\n",
      "\tap@k: 0.906435235364\n",
      "Epoch №  46\n",
      "Train:\n",
      "\tloss: 2879.63637344\n",
      "\tacc: 0.764554455446\n",
      "\tauc: 0.857400291829\n",
      "\tap@k: 0.891278333715\n",
      "Val:\n",
      "\tloss: 2289.17672275\n",
      "\tacc: 0.797623762376\n",
      "\tauc: 0.850002995954\n",
      "\tap@k: 0.720516524074\n",
      "Epoch №  47\n",
      "Train:\n",
      "\tloss: 1077.8176379\n",
      "\tacc: 0.769801980198\n",
      "\tauc: 0.857361803515\n",
      "\tap@k: 0.913732637336\n",
      "Val:\n",
      "\tloss: 3929.67478496\n",
      "\tacc: 0.737425742574\n",
      "\tauc: 0.821907488125\n",
      "\tap@k: 0.977994684793\n",
      "Epoch №  48\n",
      "Train:\n",
      "\tloss: 460.480774651\n",
      "\tacc: 0.733762376238\n",
      "\tauc: 0.841077160203\n",
      "\tap@k: 0.950153073555\n",
      "Val:\n",
      "\tloss: 2447.42338166\n",
      "\tacc: 0.75603960396\n",
      "\tauc: 0.814586615999\n",
      "\tap@k: 0.599503003174\n",
      "Epoch №  49\n",
      "Train:\n",
      "\tloss: 1289.5172763\n",
      "\tacc: 0.766435643564\n",
      "\tauc: 0.86328789296\n",
      "\tap@k: 0.871498253736\n",
      "Val:\n",
      "\tloss: 1687.4945523\n",
      "\tacc: 0.744158415842\n",
      "\tauc: 0.812925773269\n",
      "\tap@k: 0.738977278423\n",
      "Epoch №  50\n",
      "Train:\n",
      "\tloss: 257.956177818\n",
      "\tacc: 0.769702970297\n",
      "\tauc: 0.847611246571\n",
      "\tap@k: 0.905688253551\n",
      "Val:\n",
      "\tloss: 1329.47333207\n",
      "\tacc: 0.772772277228\n",
      "\tauc: 0.841579825418\n",
      "\tap@k: 0.849813249502\n",
      "Epoch №  51\n",
      "Train:\n",
      "\tloss: 374.72231102\n",
      "\tacc: 0.753267326733\n",
      "\tauc: 0.837000313818\n",
      "\tap@k: 0.932158455128\n",
      "Val:\n",
      "\tloss: 1035.87716038\n",
      "\tacc: 0.744059405941\n",
      "\tauc: 0.815105046198\n",
      "\tap@k: 0.782273905046\n",
      "Epoch №  52\n",
      "Train:\n",
      "\tloss: 358.010278441\n",
      "\tacc: 0.760891089109\n",
      "\tauc: 0.845448686843\n",
      "\tap@k: 0.909761330791\n",
      "Val:\n",
      "\tloss: 484.022222351\n",
      "\tacc: 0.761188118812\n",
      "\tauc: 0.861425061473\n",
      "\tap@k: 0.88348692765\n",
      "Epoch №  53\n",
      "Train:\n",
      "\tloss: 117.79164269\n",
      "\tacc: 0.756930693069\n",
      "\tauc: 0.84648189912\n",
      "\tap@k: 0.956021124002\n",
      "Val:\n",
      "\tloss: 1386.49773124\n",
      "\tacc: 0.776732673267\n",
      "\tauc: 0.861010617632\n",
      "\tap@k: 0.82808389182\n",
      "Epoch №  54\n",
      "Train:\n",
      "\tloss: 96.1823976201\n",
      "\tacc: 0.757128712871\n",
      "\tauc: 0.860294215678\n",
      "\tap@k: 0.949284775914\n",
      "Val:\n",
      "\tloss: 1018.45097769\n",
      "\tacc: 0.771485148515\n",
      "\tauc: 0.855056775237\n",
      "\tap@k: 0.999092592633\n",
      "Epoch №  55\n",
      "Train:\n",
      "\tloss: 37.7381571964\n",
      "\tacc: 0.759801980198\n",
      "\tauc: 0.865690679183\n",
      "\tap@k: 0.96568920921\n",
      "Val:\n",
      "\tloss: 606.539287735\n",
      "\tacc: 0.773465346535\n",
      "\tauc: 0.862599873714\n",
      "\tap@k: 0.987992717647\n",
      "Epoch №  56\n",
      "Train:\n",
      "\tloss: 6628.94217697\n",
      "\tacc: 0.74099009901\n",
      "\tauc: 0.839303186096\n",
      "\tap@k: 0.923266494361\n",
      "Val:\n",
      "\tloss: 2147.51460276\n",
      "\tacc: 0.650594059406\n",
      "\tauc: 0.810850200242\n",
      "\tap@k: 0.984440411172\n",
      "Epoch №  57\n",
      "Train:\n",
      "\tloss: 232.700540076\n",
      "\tacc: 0.68198019802\n",
      "\tauc: 0.826078616938\n",
      "\tap@k: 0.94827295009\n",
      "Val:\n",
      "\tloss: 1246.84558364\n",
      "\tacc: 0.67603960396\n",
      "\tauc: 0.842597174954\n",
      "\tap@k: 0.883145149723\n",
      "Epoch №  58\n",
      "Train:\n",
      "\tloss: 358.266601087\n",
      "\tacc: 0.704653465347\n",
      "\tauc: 0.835433991164\n",
      "\tap@k: 0.881565016855\n",
      "Val:\n",
      "\tloss: 856.229241016\n",
      "\tacc: 0.678811881188\n",
      "\tauc: 0.846760067649\n",
      "\tap@k: 0.881872267151\n",
      "Epoch №  59\n",
      "Train:\n",
      "\tloss: 68.4718308725\n",
      "\tacc: 0.689405940594\n",
      "\tauc: 0.837807210409\n",
      "\tap@k: 0.970374840136\n",
      "Val:\n",
      "\tloss: 733.37020058\n",
      "\tacc: 0.656435643564\n",
      "\tauc: 0.844546502897\n",
      "\tap@k: 0.925678126909\n",
      "Epoch №  60\n",
      "Train:\n",
      "\tloss: 168.207119041\n",
      "\tacc: 0.669801980198\n",
      "\tauc: 0.827362892388\n",
      "\tap@k: 0.958562953924\n",
      "Val:\n",
      "\tloss: 321.006132391\n",
      "\tacc: 0.64900990099\n",
      "\tauc: 0.850275903728\n",
      "\tap@k: 0.937893667415\n",
      "Epoch №  61\n",
      "Train:\n",
      "\tloss: 171.496038991\n",
      "\tacc: 0.678118811881\n",
      "\tauc: 0.83498302039\n",
      "\tap@k: 0.986406469619\n",
      "Val:\n",
      "\tloss: 1307.83516229\n",
      "\tacc: 0.678217821782\n",
      "\tauc: 0.862307374044\n",
      "\tap@k: 0.810186842574\n",
      "Epoch №  62\n",
      "Train:\n",
      "\tloss: 83.31174485\n",
      "\tacc: 0.665148514851\n",
      "\tauc: 0.825539704888\n",
      "\tap@k: 0.933486963814\n",
      "Val:\n",
      "\tloss: 168.373082983\n",
      "\tacc: 0.663861386139\n",
      "\tauc: 0.87292839193\n",
      "\tap@k: 0.93382222103\n",
      "Epoch №  63\n",
      "Train:\n",
      "\tloss: 96.9091375824\n",
      "\tacc: 0.713861386139\n",
      "\tauc: 0.83586021\n",
      "\tap@k: 0.965503894416\n",
      "Val:\n",
      "\tloss: 17.3768745672\n",
      "\tacc: 0.802673267327\n",
      "\tauc: 0.89184735186\n",
      "\tap@k: 0.98214771255\n",
      "Epoch №  64\n",
      "Train:\n",
      "\tloss: 23.2108166675\n",
      "\tacc: 0.753762376238\n",
      "\tauc: 0.850735331369\n",
      "\tap@k: 0.968368043319\n",
      "Val:\n",
      "\tloss: 26.5534742976\n",
      "\tacc: 0.76495049505\n",
      "\tauc: 0.886814254463\n",
      "\tap@k: 0.99336837687\n",
      "Epoch №  65\n",
      "Train:\n",
      "\tloss: 17.7099803042\n",
      "\tacc: 0.730396039604\n",
      "\tauc: 0.824634774192\n",
      "\tap@k: 0.965801521653\n",
      "Val:\n",
      "\tloss: 25.5252028393\n",
      "\tacc: 0.803069306931\n",
      "\tauc: 0.88585306036\n",
      "\tap@k: 0.999554866742\n",
      "Epoch №  66\n",
      "Train:\n",
      "\tloss: 104.472092306\n",
      "\tacc: 0.724851485149\n",
      "\tauc: 0.827001286194\n",
      "\tap@k: 0.941379897645\n",
      "Val:\n",
      "\tloss: 16.9276420231\n",
      "\tacc: 0.782178217822\n",
      "\tauc: 0.883009090806\n",
      "\tap@k: 0.987680072253\n",
      "Epoch №  67\n",
      "Train:\n",
      "\tloss: 13.9481435543\n",
      "\tacc: 0.761683168317\n",
      "\tauc: 0.858639358757\n",
      "\tap@k: 0.98916016244\n",
      "Val:\n",
      "\tloss: 12.0569304339\n",
      "\tacc: 0.769702970297\n",
      "\tauc: 0.895177011409\n",
      "\tap@k: 0.995337446386\n",
      "Epoch №  68\n",
      "Train:\n",
      "\tloss: 11.9580223202\n",
      "\tacc: 0.747128712871\n",
      "\tauc: 0.849259426838\n",
      "\tap@k: 0.97616703992\n",
      "Val:\n",
      "\tloss: 13.2800401675\n",
      "\tacc: 0.777524752475\n",
      "\tauc: 0.885646871041\n",
      "\tap@k: 0.979668806124\n",
      "Epoch №  69\n",
      "Train:\n",
      "\tloss: 9.07459118909\n",
      "\tacc: 0.757425742574\n",
      "\tauc: 0.848872536195\n",
      "\tap@k: 0.98533295542\n",
      "Val:\n",
      "\tloss: 19.0532029687\n",
      "\tacc: 0.807425742574\n",
      "\tauc: 0.892027010579\n",
      "\tap@k: 0.999133545483\n",
      "Epoch №  70\n",
      "Train:\n",
      "\tloss: 9.85492411731\n",
      "\tacc: 0.744257425743\n",
      "\tauc: 0.840645549712\n",
      "\tap@k: 0.954496612983\n",
      "Val:\n",
      "\tloss: 21.2104733596\n",
      "\tacc: 0.797128712871\n",
      "\tauc: 0.885616058824\n",
      "\tap@k: 0.956700019557\n",
      "Epoch №  71\n",
      "Train:\n",
      "\tloss: 7.93088263811\n",
      "\tacc: 0.759108910891\n",
      "\tauc: 0.851644863263\n",
      "\tap@k: 0.982478519503\n",
      "Val:\n",
      "\tloss: 3.02202840759\n",
      "\tacc: 0.744455445545\n",
      "\tauc: 0.887110063214\n",
      "\tap@k: 0.980970617671\n",
      "Epoch №  72\n",
      "Train:\n",
      "\tloss: 5.78221164823\n",
      "\tacc: 0.758910891089\n",
      "\tauc: 0.846185702032\n",
      "\tap@k: 0.968225060772\n",
      "Val:\n",
      "\tloss: 17.7348676774\n",
      "\tacc: 0.78099009901\n",
      "\tauc: 0.888965483194\n",
      "\tap@k: 0.997723239447\n",
      "Epoch №  73\n",
      "Train:\n",
      "\tloss: 3.48520228481\n",
      "\tacc: 0.762574257426\n",
      "\tauc: 0.849135034664\n",
      "\tap@k: 0.974948523555\n",
      "Val:\n",
      "\tloss: 3.0090676031\n",
      "\tacc: 0.802574257426\n",
      "\tauc: 0.899558245178\n",
      "\tap@k: 0.992652027283\n",
      "Epoch №  74\n",
      "Train:\n",
      "\tloss: 15.8397053059\n",
      "\tacc: 0.765841584158\n",
      "\tauc: 0.850374521377\n",
      "\tap@k: 0.96915493086\n",
      "Val:\n",
      "\tloss: 2.87183990384\n",
      "\tacc: 0.763564356436\n",
      "\tauc: 0.886866998402\n",
      "\tap@k: 0.985871402566\n",
      "Epoch №  75\n",
      "Train:\n",
      "\tloss: 3.87115106447\n",
      "\tacc: 0.757128712871\n",
      "\tauc: 0.855204153114\n",
      "\tap@k: 0.978201238175\n",
      "Val:\n",
      "\tloss: 7.89313551797\n",
      "\tacc: 0.798316831683\n",
      "\tauc: 0.890970184824\n",
      "\tap@k: 0.990406720211\n",
      "Epoch №  76\n",
      "Train:\n",
      "\tloss: 4.62206618717\n",
      "\tacc: 0.752574257426\n",
      "\tauc: 0.848708073262\n",
      "\tap@k: 0.984378450027\n",
      "Val:\n",
      "\tloss: 5.36142253713\n",
      "\tacc: 0.791584158416\n",
      "\tauc: 0.886241557986\n",
      "\tap@k: 0.983787971718\n",
      "Epoch №  77\n",
      "Train:\n",
      "\tloss: 3.77734584535\n",
      "\tacc: 0.756732673267\n",
      "\tauc: 0.850383364799\n",
      "\tap@k: 0.982626539515\n",
      "Val:\n",
      "\tloss: 1.93176040857\n",
      "\tacc: 0.798613861386\n",
      "\tauc: 0.896719633501\n",
      "\tap@k: 1.0\n",
      "Epoch №  78\n",
      "Train:\n",
      "\tloss: 0.705537254645\n",
      "\tacc: 0.761683168317\n",
      "\tauc: 0.851941352401\n",
      "\tap@k: 0.996325827532\n",
      "Val:\n",
      "\tloss: 11.3560673725\n",
      "\tacc: 0.775742574257\n",
      "\tauc: 0.888750745028\n",
      "\tap@k: 0.989361018007\n",
      "Epoch №  79\n",
      "Train:\n",
      "\tloss: 6.71662751883\n",
      "\tacc: 0.753366336634\n",
      "\tauc: 0.83937384373\n",
      "\tap@k: 0.971781919539\n",
      "Val:\n",
      "\tloss: 4.1700566065\n",
      "\tacc: 0.78\n",
      "\tauc: 0.887198936176\n",
      "\tap@k: 0.994275136162\n",
      "Epoch №  80\n",
      "Train:\n",
      "\tloss: 6.04090248374\n",
      "\tacc: 0.748415841584\n",
      "\tauc: 0.843759596635\n",
      "\tap@k: 0.971863520166\n",
      "Val:\n",
      "\tloss: 4.13764698797\n",
      "\tacc: 0.761188118812\n",
      "\tauc: 0.873241416261\n",
      "\tap@k: 0.977079205428\n",
      "Epoch №  81\n",
      "Train:\n",
      "\tloss: 2.29759166932\n",
      "\tacc: 0.759405940594\n",
      "\tauc: 0.841411716395\n",
      "\tap@k: 0.981837362447\n",
      "Val:\n",
      "\tloss: 5.524505982\n",
      "\tacc: 0.798910891089\n",
      "\tauc: 0.895121329696\n",
      "\tap@k: 0.987669261139\n",
      "Epoch №  82\n",
      "Train:\n",
      "\tloss: 1.21765643172\n",
      "\tacc: 0.759405940594\n",
      "\tauc: 0.834528858123\n",
      "\tap@k: 0.995375084568\n",
      "Val:\n",
      "\tloss: 7.95241868117\n",
      "\tacc: 0.78801980198\n",
      "\tauc: 0.893850710819\n",
      "\tap@k: 0.983230104232\n",
      "Epoch №  83\n",
      "Train:\n",
      "\tloss: 9.34016140663\n",
      "\tacc: 0.762475247525\n",
      "\tauc: 0.82997664287\n",
      "\tap@k: 0.953616112945\n",
      "Val:\n",
      "\tloss: 3.1907063442\n",
      "\tacc: 0.798316831683\n",
      "\tauc: 0.885032132783\n",
      "\tap@k: 0.989240770834\n",
      "Epoch №  84\n",
      "Train:\n",
      "\tloss: 1.51923494191\n",
      "\tacc: 0.754653465347\n",
      "\tauc: 0.826720801023\n",
      "\tap@k: 0.998022118917\n",
      "Val:\n",
      "\tloss: 1.33641957873\n",
      "\tacc: 0.793564356436\n",
      "\tauc: 0.900723649213\n",
      "\tap@k: 0.998722732939\n",
      "Epoch №  85\n",
      "Train:\n",
      "\tloss: 0.954487706028\n",
      "\tacc: 0.775148514851\n",
      "\tauc: 0.838892590943\n",
      "\tap@k: 0.980260113127\n",
      "Val:\n",
      "\tloss: 3.02963198503\n",
      "\tacc: 0.784752475248\n",
      "\tauc: 0.840889946186\n",
      "\tap@k: 0.990994912589\n",
      "Epoch №  86\n",
      "Train:\n",
      "\tloss: 0.661553592369\n",
      "\tacc: 0.761188118812\n",
      "\tauc: 0.826938354418\n",
      "\tap@k: 0.992630774764\n",
      "Val:\n",
      "\tloss: 1.03291049493\n",
      "\tacc: 0.800495049505\n",
      "\tauc: 0.897411534655\n",
      "\tap@k: 0.993983266315\n",
      "Epoch №  87\n",
      "Train:\n",
      "\tloss: 1.07895389003\n",
      "\tacc: 0.756237623762\n",
      "\tauc: 0.818300982974\n",
      "\tap@k: 0.97255970233\n",
      "Val:\n",
      "\tloss: 3.21581727804\n",
      "\tacc: 0.800891089109\n",
      "\tauc: 0.870879374805\n",
      "\tap@k: 0.98668775538\n",
      "Epoch №  88\n",
      "Train:\n",
      "\tloss: 15.3104474773\n",
      "\tacc: 0.764257425743\n",
      "\tauc: 0.827122640584\n",
      "\tap@k: 0.977758241071\n",
      "Val:\n",
      "\tloss: 11.4426812152\n",
      "\tacc: 0.807326732673\n",
      "\tauc: 0.896698578591\n",
      "\tap@k: 0.901881007756\n",
      "Epoch №  89\n",
      "Train:\n",
      "\tloss: 2.31723287854\n",
      "\tacc: 0.759801980198\n",
      "\tauc: 0.820585115704\n",
      "\tap@k: 0.964782576653\n",
      "Val:\n",
      "\tloss: 4.76010372154\n",
      "\tacc: 0.805346534653\n",
      "\tauc: 0.885709219771\n",
      "\tap@k: 0.986484548341\n",
      "Epoch №  90\n",
      "Train:\n",
      "\tloss: 0.864970826584\n",
      "\tacc: 0.767128712871\n",
      "\tauc: 0.834592398124\n",
      "\tap@k: 0.991561504073\n",
      "Val:\n",
      "\tloss: 1.68303919936\n",
      "\tacc: 0.797821782178\n",
      "\tauc: 0.899472631666\n",
      "\tap@k: 0.99124646028\n",
      "Epoch №  91\n",
      "Train:\n",
      "\tloss: 0.530647261728\n",
      "\tacc: 0.763465346535\n",
      "\tauc: 0.817871016317\n",
      "\tap@k: 0.996049027233\n",
      "Val:\n",
      "\tloss: 3.7355417075\n",
      "\tacc: 0.805247524752\n",
      "\tauc: 0.884928623539\n",
      "\tap@k: 0.959591421249\n",
      "Epoch №  92\n",
      "Train:\n",
      "\tloss: 0.640075043426\n",
      "\tacc: 0.773168316832\n",
      "\tauc: 0.832692432349\n",
      "\tap@k: 0.988277547879\n",
      "Val:\n",
      "\tloss: 1.79454660875\n",
      "\tacc: 0.783564356436\n",
      "\tauc: 0.900159369871\n",
      "\tap@k: 0.997639139411\n",
      "Epoch №  93\n",
      "Train:\n",
      "\tloss: 2.60456507364\n",
      "\tacc: 0.764554455446\n",
      "\tauc: 0.816241523177\n",
      "\tap@k: 0.969748047527\n",
      "Val:\n",
      "\tloss: 10.4526596087\n",
      "\tacc: 0.796534653465\n",
      "\tauc: 0.878786341798\n",
      "\tap@k: 0.965996384794\n",
      "Epoch №  94\n",
      "Train:\n",
      "\tloss: 0.848186677018\n",
      "\tacc: 0.77504950495\n",
      "\tauc: 0.828083450046\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 4.39230936663\n",
      "\tacc: 0.780495049505\n",
      "\tauc: 0.835177780246\n",
      "\tap@k: 0.99547580118\n",
      "Epoch №  95\n",
      "Train:\n",
      "\tloss: 2.38312037971\n",
      "\tacc: 0.760891089109\n",
      "\tauc: 0.810141771869\n",
      "\tap@k: 0.9648741419\n",
      "Val:\n",
      "\tloss: 2.98734191083\n",
      "\tacc: 0.767722772277\n",
      "\tauc: 0.862329415203\n",
      "\tap@k: 0.996753610557\n",
      "Epoch №  96\n",
      "Train:\n",
      "\tloss: 1.11287903029\n",
      "\tacc: 0.752277227723\n",
      "\tauc: 0.798625952402\n",
      "\tap@k: 0.97427741368\n",
      "Val:\n",
      "\tloss: 0.667207488151\n",
      "\tacc: 0.802376237624\n",
      "\tauc: 0.901249653212\n",
      "\tap@k: 0.994113231264\n",
      "Epoch №  97\n",
      "Train:\n",
      "\tloss: 0.803635510915\n",
      "\tacc: 0.776831683168\n",
      "\tauc: 0.825964286167\n",
      "\tap@k: 0.989812106452\n",
      "Val:\n",
      "\tloss: 7.02967947938\n",
      "\tacc: 0.8\n",
      "\tauc: 0.887673214428\n",
      "\tap@k: 0.977445618696\n",
      "Epoch №  98\n",
      "Train:\n",
      "\tloss: 0.560409152784\n",
      "\tacc: 0.76495049505\n",
      "\tauc: 0.810622311937\n",
      "\tap@k: 0.989170636882\n",
      "Val:\n",
      "\tloss: 1.11374444777\n",
      "\tacc: 0.79603960396\n",
      "\tauc: 0.894057063541\n",
      "\tap@k: 0.974760097216\n",
      "Epoch №  99\n",
      "Train:\n",
      "\tloss: 3.06273691089\n",
      "\tacc: 0.773762376238\n",
      "\tauc: 0.817572156876\n",
      "\tap@k: 0.991306131396\n",
      "Val:\n",
      "\tloss: 0.740068457727\n",
      "\tacc: 0.792277227723\n",
      "\tauc: 0.888262668676\n",
      "\tap@k: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "minibatches_per_epoch = 100\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    print \"Epoch № \", i\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Train:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Val:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \n"
     ]
    }
   ],
   "source": [
    "print \"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"nn100.pcl\",'w') as fout:\n",
    "    pickle.dump(nn,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"nn100.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "\tloss: 1.66862034093\n",
      "\tacc: 0.791929437707\n",
      "\tauc: 0.884823213418\n",
      "\tap@k: 0.987654145506\n",
      "\n",
      "AUC:\n",
      "\tТы на правильном пути! (not ok)\n",
      "\n",
      "Accuracy:\n",
      "Надо бы подтянуть. (not ok)\n",
      "\n",
      "Average precision at K:\n",
      "\tОтличный результат (good)\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print \"Scores:\"\n",
    "print '\\tloss:',b_loss/b_c\n",
    "print '\\tacc:',final_accuracy\n",
    "print '\\tauc:',final_auc\n",
    "print '\\tap@k:',final_apatk\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main task\n",
    "\n",
    "* https://goo.gl/forms/eJwIeAbjxzVuo6vn1\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
